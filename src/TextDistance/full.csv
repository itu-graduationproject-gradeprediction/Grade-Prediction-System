id;answer
108;I tested all test data with the trained model’s output formula which can be seen below. Output = 1/(1 + e-(weight1*feature1 + weight2*feature2 + bias)) My success criteria is if one test data’s label is 1, it’s output must be equal or bigger than 0.5 and if one test data’s label is 0, it’s output must be smaller than 0.5. 
111;I wrote a discriminant function called discriminant which takes sample features, mean vector, covariance vector and class probability as parameters. I calculated discriminant values of all samples in classification_test dataset one by one. I compared the results of discriminant values. If the discriminant value of sample with class0 vectors is greater than class1 then I predicted 0 for that sample and vice versa.
112;For each test sample, discriminant function value is calculated for label 0 and label 1. If value for label 0 is bigger than label 1, sample is predicted as 0 otherwise 1. These results stored in y prediction vector.
114;Result of classification can be found when the part2_d.py file run. Classification result is stored in the classification list. 
115;325 testing sample has been predicted correctly.
117;I used the non-simplified form because I had different cases for each discriminant function class. 
118;The discriminant functions together correspond to a line between the distributions of 2 classes and they each predict the possibility of a point belonging to their class. 
119;I compared g1(x) and g2(x) for all testing samples and filled class labels for predicted table according to discriminant functions. I used g functions above. If g1(x) > g2(x), predicted label is 0, else predicted label is 1. 
120;Discriminant functions are used to predict labels of testing samples from classification_test.txt. If g1(xtst) ≥ g2(xtst) the predicted testing label is c1 else c2. 
122;Discriminant functions calculate a score for given features. So I calculated a score for each class with given input. Then I compared results and made a prediction about test data. With this mathmetical operations,I calculated correct prediction 354 for 400 test cases. 
124;Discriminant functions are used to predict labels of testing samples from classification_test.txt. If g1(x_tst) ≥ g2(x_tst) the predicted testing label is 0 else 1. 
126;I tested my classifier using classification_test dataset for both class 1 and class 2 discriminant functions computed before. They classified as class 1 if g1(xi) >= g2(xi), else classified as class 2.
130;In this part, I added new column to data, about my quess. If g1(x)>=g2(x), I added 1 else 0. 
131;"In order to predict label of each testing sample, discriminant functions are used. If g1(x) ≥ g2(x) then label is c1, otherwise; c2. Thus, It has been observed that how much the model has learned from the data. "
132;Correct Prediction is 354 
133;Classification can be achieved by the discriminant function.
135;The discriminant functions determine the test instances class by using the mean vector and covariance matrix of their class. We can determine the given instance’s class with comparing the discriminant functions of the classes. Which discriminant function gives the high result with that instance, will be the class of the given instance.
137;All the test samples are sent to the functions g1 and g2 row by row. These two function values are calculated and the greater one’s label is chosen as the prediction. For label 0 prediction, g1 is used and for label 1 prediction g2 function is used. For g1, mean vector of zero label (meanvec_zero), covariance matrice of zero label (covmat_zero) and probability of zero label(zero_prob) are used. For g2, meanvec_one cov_mat_one an done_prob are used.
139;Negative and positive examples in the data cannot be divided by a line, hence the non-zero values in the minor diagonals in the covariance matrices. The negative and positive examples are roughly in the same numbers, thus the probability of the classes are about the same. For the most part, the program can classify examples it has not seen, so we can say it generalizes well.
144;What we are essentially doing when we calculate the discriminant functions is using Bayes Rule to calculate the possibility of the class label being i when the x value is given. Thus if the discriminant function of c1 is larger than c2 then this x value has a larger probability of belonging to the c1 class.
145;My prediction is generated for each sample in test data by computing and comparing two discriminant functions written above (g1(x) and g2(x)). My python implementation file is named “Part2.py”. 
146;We compare g1(xtst) and g2(xtst)  for all test inputs. If g1(xtst) >= g2(xtst) , predicted label is 1 otherwise predicted label is 0. The z matrix (has 400 rows, 1 column) in the lfd2.m file is the prediction label matrix. In  the next step we will compare predicted labels and actual labels.
148;g 1(x) and g 2(x) values of testing samples are gathered. Each discriminant function has 2 values because of 2 features. So magnitude of g1 and g2 are compared. Whichever larger, gets the data to its class.
149;First, I calcuate the discriminant function of feature data. Then compare both of the results to decide data is in class1 or class2. I make a prediction with choosing the higher discriminant function. Then checked if my prediction is correct or not. In %88.5 of the cases, it predict the right class.
151;Discriminant function is negative square of distance from pattarn point to mean square of each class. If distance of first class is grater than second class pattern is in second class. It means g1(x) >= g2(x) pattern is in first class.
152;My implementation sends every test input into discriminant function, stores probability of y being c1 into y_pred_lbl1, c2 into y_pred_lbl2. Y_all vector is created from y_pred_lbl1 and y_pred_lbl2. If probability of label 2 is higher, then the value should be 1(True). Therefore I have compared it as y_all = (y_pred_lbl1 < y_pred_lbl2). Multiplication by 1 transforms boolean vector to integer one. 
154;In this part, I used my discriminant function implementation over the test data sample to calculate discriminant function results for each class label. Then I calculated predictions by comparing these results for each test data samples. If a sample’s result of discriminant function for c1 is greater than its result of discriminant function for c2, then prediction for this sample is 0, if smaller, then prediction for this sample is 1. 
156;Correct ones are 354 pieces.
157;"Using the discriminant function formulas g1(x) and g2(x) that were calculated in c), using the features in classification_test.txt  (g(x) has two inputs, x is a vector).  If the value of g1(xi) >= g2(xi) then the prediction for xi is 0.  Else the prediction for xi is 1.
program used: partcd.py"
159;For 400 instances in the test set a new value p for prediction was calculated through the discriminant functions. For every instance the two features were given as an input to the functions and the output was 1 or 0. Afterwards this value was compered with the given label. 
2;"There are 131 false labels out of 400. 116 of them are labeled as 0 but they should be labeled as 1. Therefore; errors are mostly belongs to class 1. Classifier is better at classifying class 0 data."
3;After training our model(i.e obtaining implicit/ready-to-use discriminant functions g i (x), which requires covariance matrix, determinant of covariance matrix, mean vector and probability of each class), we try to predict label of each data sample in testing set using these models. Subsequently, we compare the expected result(estimated labels of testing data) with actual data(real labels of testing data), and determine overall accuracy(# of correctly predicted samples / total # of samples).
6;For each testing sample from testing set (classification_test.txt), prediction was made by the discrimination function that is constructed in hw1_2.py file. The results can be observed from the output of the program along with table similar to which is provided in HW1.pdf file.
8;I’m checking the functions for all features of al samples if which one is bigger. If g1(x) is bigger than g2(x) then, its label is 0, otherwise 1.
9;I got the predictions with “makePrediction” function. It takes feature1 and feature2, then it passes features to discriminant functions, and makes prediction looking for which discriminant is larger.
15;"I have printed predicted labels on ""prediction_test.txt"" file"
16;Discriminant functions are derived from bayes probability rule. By using this rule, we synthesized two discriminant functions for our classes. Each function takes a sample as an input and make a probabilistic prediction. To decide which class a data sample belongs to, we have to substitute that sample in all discriminant functions, and examine the probability values generated by these functions. If the biggest probable value is generated by discriminant function of class i, that sample belongs to that class. By using this bayes classifier notion, I compared probability values of two discriminant functions for all samples, and I decided, by examining the result of this comparison, which class those sample belongs to. The predictions and ground truth labels are written to the file “Classifier Predictions.txt”. This operation is performed by a piece of codes in python file “classification_results.py”. You can also see and examine the predictions with the help of this file.
17;I stored the values of feat1, feat2 and label on 3 different list. Then calculated values of g1 and g2, compared them and stored the result in guessed label list. For the code: part2d-e.py part2d-e.txt
19;Each test sample is predicted according to the discriminant functions in part-c. If g 1 ( x tst ) ≥ g 2 ( x tst ) and y tst = 0, it is accepted as a correct prediction, or vice versa.
20;Using Bayes discriminant functions g i (x)=p(x|ci)*p(ci) , the probabilities of belonging to a class is calculated for each class’ discriminant function and these probabilities are compared to decide which class the data point belong to.
22;Total of 172 items are predicted as class 1, remaining 228 items are predicted as class 2.
24;I was not able to complete this part.
25;I calculated discriminant function for class 1 and class 2 for every sample in the classification_test.txt file. If g_i > g_j, the sample belongs to class i. I compared the results with the real labels. Out of 400 testing samples 354 samples were labeled correctly.
27;Classifier prediction is 88.75% correct The function also can be used for different datasets. But the variables should be calculated before. It returns a value to compare with the variables for other class. The prediction is done regarding the greatness of the return values.
30;I tested every sample on classification_test.txt file and compared g1 and g2 values. If g1>g2, then it’s expected label is 0, otherwise 1.
32;After giving an input, predict() function calculates gg ii for each class and calculates argmax to determine which class has higher likelihood.
33;Computed gi(x) values and then compared two of them and labeled with the class whose discriminant function is higher.
34;I found discriminant for class1 and class2 then i compare my results and test label results.
36;Since the covariances of features for both of the classes are distributed in an eliptic manner, the Mahalanobis distance is used for discriminant function. Then, for both of the classes discriminant function results of the test set samples are calculated. The greater one is assumed as the predicted value.
37;Since the covariances and means of both classes are different from each-other, the discriminant separating the classes will be non-linear.
38;When first i printed comparison vector i thought that my discriminant function is incorrect due to the some comparisons resulted as false. Right after i realized that data points of class1 and class2 are blended together so it is redundant to predict each point’s real class truly. Mostly correct results are enough.
39;There are two ways of implementation discrimination function one of them is using organized formula another is using p(x|ci) * p(ci). My implementation does second one. For the test data it checks whether g 1 (x) > g 2 (x), if so data is from class1 otherwise class2. This function just needs covariance and mean matrices of data grouped as labels.
42;From the calculations, classifier predicted 172 c1’s and 226 c2’s. Further analysis and explanation is given in next section.
44;In classifier.py Discriminant.test_model () is predicting and measuring accuracy. However program did not write its result down but if code and accuracy result are examined, its predictions can be realized.
47;I tested all given data and calculated an accuracy.
49;The discriminant function was formed by taking the natural logarithm of the probability P(C i |x) which was equal to the multiplication of P(x|C i ) and P(C i ). The probability P(x|C i ) was taken as a Gaussian curve and again it’s natural logarithm was used. I used the discriminant function g i (x)to predict the test data. The probability of a class occuring (P(C i )), mean and covariance matrix of a class was given as a parameter for each class. Samples were predicted as the class label which returned the greatest value from the function g i .
50;I used the quadratic approach to get Q,L,K and since they are related to the mean and covariance of the classes, the results are different.
55;Discriminant formula is used for figuring out the line between two given classes. When we compared the discriminant results of class1 and class2, we could determine the correctness of predictions since the one with the bigger g(x) includes the given data point (we checked from the label if its matched).
57;I get above result as an predict in test stage. According discriminant function result which is probability of belonging to class according to given feature vector, my function selects the higher probability and assign a label for each sample in the test file.
58;After writing discriminant function, I sent a sample to the function 2 times, first with the first class’ parameters and secondly with the second one’s parameters. So, I evaluated both results and compared them. The higher is my prediction.
60;I can not do this part.
62;I have tested each testing sample using discrimant funstions and compared them wtih truth label.
63;The classes in the dataset have covariance matrices different from one another therefore I have used the quadratic formula for the calculation of the discriminant functions.g i (x)= (-1/2)(x - μ i ) t Σ i-1 ( x – μ i ) – (1/2)ln(|ΣΣ i-1 |Σ) + ln(P(c i )) This means that the classes cannot be separated linearly, as we can also see from the plot above.
65;I decided to use the formula Gi = (-1/2)(x - μ i ) t Σ i-1 ( x – μ i ) – (1/2)ln(|Σ i |) + ln(P(c i )) to calculate the discriminant functions. The reason why I chose this formula is because the classes in my dataset fulfill the third condition, which is that they have unequal covariance matrices. The covariance matrices are neither multiples of the identity matrix, nor are they equal to each other. Therefore, the quadratic formula seems to be the best choice.
66;Upon predicting the label of 400 testing data samples, the result obtained is 172 of the testing samples are predicted to belong to class C1 and 228 to class C2.
67;The results I achieved came from finding discriminant function of class one and class zero written above.
68;After calculating discriminant functions for each class, I applied the algorithm required for the predicted labels. Some predicted labels contradict with the actual ones.
69;For 400 instances in the test set a new value p for prediction was calculated through the discriminant functions. For every instance the two features were given as an input to the functions and the output was 1 or 0. Afterwards this value was compered with the given label.
70;The discriminant function is actually a multivariate normal probability density function. It calculates the probabilty of given x-y pair. The function of a class outputting the highest probability among all class functions for a given pair (test) is used to classify the given pair in that class.
71;I assume that sample with label 1 is belong to class one and the others belong to class two. I used discriminant function to determine which class sample belong to.If g 1 (x) >= g 2 (x), Sample belongs to class one. Otherwise, it belongs to class two.
72;I tested each testing data using the discriminant functions I wrote, I realized that for some data, calculated label was not the same with actual label. I counted the number of times my discriminant function was correct, and by that I calculated accuracy.
74;I found the discriminant function of two classes using mean vector, covariance matrix and probabilities of each class. Discriminant function is a function that separates two or more classes.