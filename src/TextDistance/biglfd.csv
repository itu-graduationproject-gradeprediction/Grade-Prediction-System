id;answer
108;I tested all test data with the trained model’s output formula which can be seen below. Output = 1/(1 + e-(weight1*feature1 + weight2*feature2 + bias)) My success criteria is if one test data’s label is 1, it’s output must be equal or bigger than 0.5 and if one test data’s label is 0, it’s output must be smaller than 0.5.
111;I wrote a discriminant function called discriminant which takes sample features, mean vector, covariance vector and class probability as parameters. I calculated discriminant values of all samples in classification_test dataset one by one. I compared the results of discriminant values. If the discriminant value of sample with class0 vectors is greater than class1 then I predicted 0 for that sample and vice versa.
112;For each test sample, discriminant function value is calculated for label 0 and label 1. If value for label 0 is bigger than label 1, sample is predicted as 0 otherwise 1. These results stored in y prediction vector.
114;Result of classification can be found when the part2_d.py file run. Classification result is stored in the classification list.
115;325 testing sample has been predicted correctly.
117;I used the non-simplified form because I had different cases for each discriminant function class. 
118;The discriminant functions together correspond to a line between the distributions of 2 classes and they each predict the possibility of a point belonging to their class.
119;I compared g1(x) and g2(x) for all testing samples and filled class labels for predicted table according to discriminant functions. I used g functions above. If g1(x) > g2(x), predicted label is 0, else predicted label is 1.
120;Discriminant functions are used to predict labels of testing samples from classification_test.txt. If g1(xtst) ≥ g2(xtst) the predicted testing label is c1 else c2.
122;Discriminant functions calculate a score for given features. So I calculated a score for each class with given input. Then I compared results and made a prediction about test data. With this mathmetical operations,I calculated correct prediction 354 for 400 test cases.
124;Discriminant functions are used to predict labels of testing samples from classification_test.txt. If g1(x_tst) ≥ g2(x_tst) the predicted testing label is 0 else 1.
126;I tested my classifier using classification_test dataset for both class 1 and class 2 discriminant functions computed before. They classified as class 1 if g1(xi) >= g2(xi), else classified as class 2.
130;In this part, I added new column to data, about my quess. If g1(x)>=g2(x), I added 1 else 0.
131;"In order to predict label of each testing sample, discriminant functions are used. If g1(x) ≥ g2(x) then label is c1, otherwise; c2. Thus, It has been observed that how much the model has learned from the data."
3;After training our model(i.e obtaining implicit/ready-to-use discriminant functions g i (x), which requires covariance matrix, determinant of covariance matrix, mean vector and probability of each class), we try to predict label of each data sample in testing set using these models. Subsequently, we compare the expected result(estimated labels of testing data) with actual data(real labels of testing data), and determine overall accuracy(# of correctly predicted samples / total # of samples).
6;For each testing sample from testing set (classification_test.txt), prediction was made by the discrimination function that is constructed in hw1_2.py file. The results can be observed from the output of the program along with table similar to which is provided in HW1.pdf file.
8;I’m checking the functions for all features of al samples if which one is bigger. If g1(x) is bigger than g2(x) then, its label is 0, otherwise 1.
9;I got the predictions with “makePrediction” function. It takes feature1 and feature2, then it passes features to discriminant functions, and makes prediction looking for which discriminant is larger.
15;"I have printed predicted labels on ""prediction_test.txt"" file"
16;Discriminant functions are derived from bayes probability rule. By using this rule, we synthesized two discriminant functions for our classes. Each function takes a sample as an input and make a probabilistic prediction. To decide which class a data sample belongs to, we have to substitute that sample in all discriminant functions, and examine the probability values generated by these functions. If the biggest probable value is generated by discriminant function of class i, that sample belongs to that class. By using this bayes classifier notion, I compared probability values of two discriminant functions for all samples, and I decided, by examining the result of this comparison, which class those sample belongs to. The predictions and ground truth labels are written to the file “Classifier Predictions.txt”. This operation is performed by a piece of codes in python file “classification_results.py”. You can also see and examine the predictions with the help of this file.
17;I stored the values of feat1, feat2 and label on 3 different list. Then calculated values of g1 and g2, compared them and stored the result in guessed label list. For the code: part2d-e.py part2d-e.txt
19;Each test sample is predicted according to the discriminant functions in part-c. If g 1 ( x tst ) ≥ g 2 ( x tst ) and y tst = 0, it is accepted as a correct prediction, or vice versa.
20;Using Bayes discriminant functions g i (x)=p(x|ci)*p(ci) , the probabilities of belonging to a class is calculated for each class’ discriminant function and these probabilities are compared to decide which class the data point belong to.